# TCP和UDP的区别和应用场景
UDP：无连接的，尽最大努力交付，面向报文的，没有拥塞控制和流量控制，支持一对一，一对多，多对一和多对多的交互通信，传输协议比较简单，实时性和工作效率较高
TCP:  面向连接的，提供可靠交付，面向字节流，只支持一对一通信，提供全双工通信；
适用场景：
* TCP:
**对数据传输的质量有较高要求，但对实时性要求不高。比如HTTP，HTTPS，FTP等传输文件的协议以及POP，SMTP等邮件传输的协议，应选用TCP协议。**
UDP:
**只对数据传输的实时性要求较高，但不对传输质量有要求。比如视频传输、实时通信等，应选用UDP协议**
# TCP如何保证可靠传输
* 校验和
发送的数据包的二进制相加然后取反，目的是检测数据在传输过程中的任何变化。如果收到段的检验和有差错，TCP将丢弃这个报文段和不确认收到此报文段。
* 确认应答+序列号TCP给发送的每一个包进行编号，接收方对数据包进行排序，把有序数据传送给应用层。
* 超时重传
当TCP发出一个段后，它启动一个定时器，等待目的端确认收到这个报文段。如果不能及时收到一个确认，将重发这个报文段。
* 流量控制
TCP连接的每一方都有固定大小的缓冲空间，TCP的接收端只允许发送端发送接收端缓冲区能接纳的数据。接收方有即时窗口（滑动窗口），随ACK报文发送![5e1267ff2ed886b36ddbbaaf2ac1713b.png](en-resource://database/1168:2)
当接收方来不及处理发送方的数据，能提示发送方降低发送的速率，防止包丢失。TCP使用的流量控制协议是可变大小的滑动窗口协议。

* 拥塞控制
当网络拥塞时，减少数据的发送。发送方有拥塞窗口，发送数据前比对接收方发过来的即使窗口，取小
# UDP如何实现TCP可靠传输
因为UDP是无连接的协议，所以在传输层上无法保证可靠传输，要想实现可靠传输，只能从应用层实现。需要实现seq/ack机制，重传机制和窗口确认机制。就要接收方收到UDP之后回复个确认包，发送方有个机制，收不到确认包就要重新发送，每个包有递增的序号，接收方发现中间丢了包就要发重传请求，当网络太差时候频繁丢包，防止越丢包越重传的恶性循环，要有个发送窗口的限制，发送窗口的大小根据网络传输情况调整，调整算法要有一定自适应性。
# 端口的作用是什么
端口的作用是对TCP/IP体系的应用进程进行统一的标志，使运行不同操作系统的计算机的应用进程能够相互通信。

# Mac地址与IP地址区别
一、地址性质不同
MAC地址是物理地址，IP地址是逻辑地址。MAC地址工作在数据链路层，IP地址工作在网络层，是给每个连接在因特网的主机分配一个在全世界范围内唯一的标识符，从而可以把整个因特网看成一个单一的抽象的网络。
二、地址长度不同
1. mac地址的长度为48位（6个字节），通常表示为12个16进制数，每2个16进制数之间用冒号隔开
2.ip地址为32位，由用点分隔开的4个8位组构成，如192.168.0.1就是一个ip地址，这种写法叫点分十进制格式。
3.分配依据不同
1.MAC地址的分配是由网络设备制造商生产时写在硬件内部。这个地址与网络无关，也就是说无论将带有这个地址的硬件（如集线器、网卡、路由器等）接入到网络的何处，他都有相同的MAC地址，是不可变的
2.IP地址的分配是基于网络拓补
IP地址由网络地址和主机地址两部分组成，分配给这两部分的位数随地址类（A类、B类、C类）的不同而不同。
# ARP解析过程
网络层使用的是IP地址，但实际网络的链路上传送数据帧时，最终还是使用该网络的硬件地址，ARP协议主要解决的就是知道了一个机器的IP地址如何寻找他的硬件地址。
每一个主机都设有一个ARP高速缓存，里面有本局域网上的各主机和路由器的IP地址到硬件地址的映射表，当主机A要向本局域网的主机B发送数据报时，首先在ARP告诉缓存中查找有无对应的主机B的IP地址，如果有就在ARP高速缓存中找出其对应的硬件地址，再把该硬件地址写入MAC帧，通过局域网把该MAC帧发往此硬件地址；如果没有查到主机B的IP地址项目，主机A就自动进行ARP，ARP进程向本局域网发送ARP请求分组，请求分组的主要内容是：“我的IP地址是多少，我的硬件地址是多少，想知道IP地址是多少的硬件地址。”本局域网所有的主机上运行的ARP进程都会收到此ARP请求分组，如果主机B的ip地址与请求分组想要查询的IP地址一致，就向主机A发送自己的ARP响应分组，响应分组的内容是“我的ip地址是多少，我的硬件地址是多少”响应分组单播，同时也会把主机A的IP地址和硬件地址的映射存入自己的ARP高速缓存，如果主机的IP地址与ARP请求分组的IP地址不一致，则不理睬这个请求分组。主机A收到响应分组就在ARP缓存中写入主机B的IP地址到硬件地址的映射。
ARP把保存在高速缓存中的每一个映射地址项目都设置一个生存时间，解决局域网上主机硬件地址的可能发生变化的情况。

# DNS原理
![5d73cba92c25aca532b46388a3aeb81c.png](en-resource://database/1162:1)
一、主机向本地域名服务器的查询一般都是采用递归查询。所谓递归查询就是：如果主机所询问的本地域名服务器不知道被查询的域名的IP地址，那么本地域名服务器就以DNS客户的身份，向其它根域名服务器继续发出查询请求报文(即替主机继续查询)，而不是让主机自己进行下一步查询。因此，递归查询返回的查询结果或者是所要查询的IP地址，或者是报错，表示无法查询到所需的IP地址。
二、本地域名服务器向根域名服务器的查询通常是采用迭代查询。迭代查询的特点：当根域名服务器收到本地域名服务器发出的迭代查询请求报文时，要么给出所要查询的IP地址，要么告诉本地服务器：“你下一步应当向哪一个域名服务器进行查询”。然后让本地服务器进行后续的查询（而不是替本地域名服务器进行后续查询）。根域名服务器通常是把自己知道的顶级域名服务器的IP地址告诉本地域名服务器，让本地域名服务器再向顶级域名服务器查询。顶级域名服务器在收到本地域名服务器的查询请求后，要么给出所要查询的IP地址，要么告诉本地服务器下一步应当向哪一个权限域名服务器进行查询。最后，知道了所要解析的IP地址或报错，然后把这个结果返回给发起查询的主机
为了提高DNS查询效率，并减轻根域名服务器的负荷和减少因特网上DNS查询报文的数量，在域名服务器中广泛的使用了高速缓存，高速缓存用来存放最近查询过的域名以及从何处获得域名映射信息的记录。实际解析过程中，
1、若没有，则搜索操作系统中的DNS缓存（维护一张域名与IP地址的对应表）；　　
2、若没有，则搜索操作系统的hosts文件（Windows环境下，维护一张域名与IP地址的对应表）；　　
3、若没有，则操作系统将域名发送至本地域名服务器---（递归查询方式），本地域名服务器查询自己的DNS缓存，查找成功则返回结果，否则，（以下是迭代查询方式）　　　　
    3.1. 本地域名服务器向根域名服务器（其虽然没有每个域名的具体信息，但存储了负责每个域，如com、net、org等解析的顶级域名服务器的地址）发起请求，此处，根域名服务器返回com域的顶级域名服务器的地址；　　　　
    3.2. 本地域名服务器向com域的顶级域名服务器发起请求，返回baidu.com权限域名服务器（权限域名服务器，用来保存该区中的所有主机域名到IP地址的映射）地址；　　　　
    3.3. 本地域名服务器向baidu.com权限域名服务器发起请求，得到www.baidu.com的IP地址；　　
4、本地域名服务器将得到的IP地址返回给操作系统，同时自己也将IP地址缓存起来；　　
5、操作系统将IP地址返回给浏览器，同时自己也将IP地址缓存起来；　　
6、至此，浏览器已经得到了域名对应的IP地址；
# HTTP状态码
![6185f03e0cf03607bea6bda2747fd630.png](en-resource://database/1163:1)
![10f96eac1753f9879864a99d7df95b79.png](en-resource://database/1130:1)
# 原理 
HTTP协议定义Web客户端如何从Web服务器请求Web页面，以及服务器如何把Web页面传送给客户端。HTTP协议采用了请求/响应模型。客户端向服务器发送一个请求报文，请求报文包含请求的方法、URL、协议版本、请求头部和请求数据。服务器以一个状态行作为响应，响应的内容包括协议的版本、成功或者错误代码、服务器信息、响应头部和响应数据。
# 一次完整的HTTP请求过程
* 域名解析
* 为了将消息从客户端上传到服务器，需要用到IP协议，ARP协议和OSPF协议
* 发起TCP的三次握手，建立TCP连接
* 建立TCP连接后发起HTTP请求
* 服务器响应HTTP请求
* 浏览器解析html代码，并请求HTML代码中的资源
* 断开TCP连接
* 浏览器对页面进行渲染呈现给用户
# UDP首部格式
![64f2aa47ab5b3aa12234d390f2b5e883.png](en-resource://database/1171:1)
# TCP首部格式
![55a91b40eedf984dea3ce67d72d02858.png](en-resource://database/1167:1)
![167107c791c9876a9f5519d01a309498.png](en-resource://database/1164:1)

# TCP三次握手、四次挥手*
**刚开始客户端处于 closed 的状态，服务端处于 listen 状态**。然后
      1、第一次握手：客户端给服务端发一个 连接请求报文，此时SYN=1,ACK=0,初始化序列号 x。此时客户端处于 SYN_Send 状态。
      2、第二次握手：服务器收到客户端的连接请求报文之后，如果同意建立连接，会向客户端发送连接确认报文段，SYN=1,ACK=1,确认号为x+1,并且也是指定了自己的初始化序列号 y,此时服务器处于 SYN_REVD 的状态。
      3、第三次握手：客户端收到连接确认报文之后，还会向服务器发送确认，确认号为y+1，序列号为x+1,此时客户端处于 establised 状态。
      4、服务器收到 ACK 报文之后，也处于 establised 状态，此时，双方以建立起了链接。

* 三次握手的作用
1、确认双方的接受能力、发送能力是否正常。      
2、指定自己的初始化序列号，为后面的可靠传送做准备。     
3、如果是 https 协议的话，三次握手这个过程，还会进行数字证书的验证以及加密密钥的生成到。
4. 防止失效的连接请求到达服务器，让服务器错误打开连接
* 三次握手过程中可以携带数据吗
第一次第二次不可以，其实第三次握手的时候，是可以携带数据的。也就是说，第一次、第二次握手不可以携带数据，而第三次握手是可以携带数据的。
假如第一次握手可以携带数据的话，如果有人要恶意攻击服务器，那他每次都在第一次握手中的 SYN 报文中放入大量的数据，因为攻击者根本就不理服务器的接收、发送能力是否正常，然后疯狂着重复发 SYN 报文的话，这会让服务器花费很多时间、内存空间来接收这些报文。也就是说，第一次握手可以放数据的话，其中一个简单的原因就是会让服务器更加容易受到攻击了。
而对于第三次的话，此时客户端已经处于 established 状态，也就是说，对于客户端来说，他已经建立起连接了，并且也已经知道服务器的接收、发送能力是正常的了，所以能携带数据页没啥毛病。
* 四次挥手
![d799ccfcf2690e3480252b8b0a89fa14.png](en-resource://database/1174:1)
 刚开始双方都处于 establised 状态，假如是客户端先发起关闭请求，则：
      1、第一次挥手：客户端发送一个 FIN 报文，报文中会指定一个序列号。此时客户端处于FIN_WAIT1状态。
      2、第二次握手：服务端收到 FIN 之后，会发送 ACK 报文，且把客户端的序列号值 + 1 作为 ACK 报文的序列号值，表明已经收到客户端的报文了，此时服务端处于 CLOSE_WAIT状态。
      3、第三次挥手：如果服务端也想断开连接了，和客户端的第一次挥手一样，发给 FIN 报文，且指定一个序列号。此时服务端处于 LAST_ACK 的状态。
      4、第四次挥手：客户端收到 FIN 之后，一样发送一个 ACK 报文作为应答，且把服务端的序列号值 + 1 作为自己 ACK 报文的序列号值，此时客户端处于 TIME_WAIT 状态。需要过一阵子以确保服务端收到自己的 ACK 报文之后才会进入 CLOSED 状态
      5、服务端收到 ACK 报文之后，就处于关闭连接了，处于 CLOSED 状态。
![a22c7cd5c117ee70e5bf0435ac1e2b5e.png](en-resource://database/1173:1)
# 服务器出现大量close_wait的连接的原因以及解决方法
close_wait状态是在TCP四次挥手的时候收到FIN但是没有发送自己的FIN时出现的，服务器出现大量close_wait状态的原因有两种：

* 服务器内部业务处理占用了过多时间，都没能处理完业务；或者还有数据需要发送；或者服务器的业务逻辑有问题，没有执行close()方法
* 服务器的父进程派生出子进程，子进程继承了socket，收到FIN的时候子进程处理但父进程没有处理该信号，导致socket的引用不为0无法回收
**处理方法：**

* 停止应用程序
* 修改程序里的bug

# TCP状态转换
![43ae6472cbb15ad5811e620f651f8a03.png](en-resource://database/1166:1)
# TCP状态中TIME_WAIT
2MSL,2个报文段最大生存时间
![5f68530991433eebc9964a01fde3501a.png](en-resource://database/1170:1)
# TCP第三次握手失败会出现什么
当第三次握手失败时，服务器并不会重传ack报文，而是直接发送RST报文段，进入closed状态，这样做的目的是为了防止SYN洪泛攻击。
# TCP长连接和短链接及优缺点
![5ef51afaa731ed3f04dd9e79f590254a.png](en-resource://database/1169:1)
![69af7116846b98ccb4608b2e627b17ba.png](en-resource://database/1172:1)
![2f6c1c22e7c4c49a12a02c50be1cb85b.png](en-resource://database/1165:1)
# TCP拥塞控制-慢启动、拥塞避免、快重传、快启动
1. 慢开始
慢启动算法的思路是当主机开始发送数据时，先以比较小的拥塞窗口进行发送，然后每次翻倍，也就是说，由小到大逐渐增加拥塞窗口的大小，而这个大小是指数增长的，即1、2、4、8、16为了防止拥塞窗口cwnd增长过大引起网络拥塞，还要另外设置一个慢启动阈值ssthresh状态变量，当拥塞窗口的大小超过慢启动阈值的时候（ cwnd > ssthresh 时），停止使用慢开始算法而改用拥塞避免算法
2. 拥塞避免
拥塞避免算法的思路是让拥塞窗口cwnd缓慢地增大，即每经过一个往返时间RTT就把发送方的拥塞窗口cwnd加1，而不是加倍。
3. 快速重传
快重传算法首先要求接受方每收到一个失序的报文段就立即发出重复确认，快重传算法规定，发送方只要一连收到三个重复确认就应当立即重传对方尚未收到的报文段。
4. 快速恢复
当发送方连续收到三个重复确认后，就把慢开始门限减半，这是预防网络发生拥塞
但接下来不执行慢开始算法，由于发送方认为现在还没有发生拥塞，如果发生严重拥塞，就不会有连续好几个报文段连续到达接受方，因此与慢开始算法不同之初是把拥塞窗口的值设置为慢开始门限的一半，开始执行拥塞避免算法。
# TCP如何解决粘包、拆包问题
因为TCP是面向流，没有边界，所以接收端在一次接收的时候有可能一次接收多个包。而TCP粘包就是发送方的若干个数据包到达接收方的时候粘成了一个包。多个包首尾相接，无法区分。
如果一次请求发送的数据量比较小，没达到缓冲区大小，TCP则会将多个请求合并为同一个请求进行发送，这就形成了粘包问题。
如果一次请求发送的数据量比较大，超过了缓冲区大小，TCP就会将其拆分为多次发送，这就是拆包。
导致TCP粘包的原因有三方面：
* 发送端等待缓冲区满才进行发送，造成粘包
* 接收方来不及接收缓冲区内的数据，造成粘包
* 由于TCP协议在发送较小的数据包的时候，会将几个包合成一个包后发送
![5e1267ff2ed886b36ddbbaaf2ac1713b.png](en-resource://database/1168:2)
# 滑动窗口

* TCP 利用滑动窗口实现流量控制的机制。
* TCP 中采用滑动窗口来进行传输控制，滑动窗口的大小意味着接收方还有多大的缓冲区可以用于接收数据。发送方可以通过滑动窗口的大小来确定应该发送多少字节的数据。当滑动窗口为 0 时，发送方一般不能再发送数据报，但有两种情况除外，一种情况是可以发送紧急数据，例如，允许用户终止在远端机上的运行进程。另一种情况是发送方可以发送一个 1 字节的数据报来通知接收方重新声明它希望接收的下一字节及发送方的滑动窗口大小。
* 流量控制TCP 利用滑动窗口实现流量控制。流量控制是为了控制发送方发送速率，保证接收方来得及接收。接收方发送的确认报文中的窗口字段可以用来控制发送方窗口大小，从而影响发送方的发送速率。将窗口字段设置为 0，则发送方不能发送数据。、

# 1.Q：什么是TCP粘包问题？
TCP粘包就是指发送方发送的若干包数据到达接收方时粘成了一包，从接收缓冲区来看，后一包数据的头紧接着前一包数据的尾，出现粘包的原因是多方面的，可能是来自发送方，也可能是来自接收方。
# 2.Q：造成TCP粘包的原因
（1）发送方原因TCP默认使用Nagle算法（主要作用：减少网络中报文段的数量），而Nagle算法主要做两件事：

    1. 只有上一个分组得到确认，才会发送下一个分组收集多个小分组，在一个确认到来时一起发送Nagle算法造成了发送方可能会出现粘包问题
（2）接收方原因TCP接收到数据包时，并不会马上交到应用层进行处理，或者说应用层并不会立即处理。实际上，TCP将接收到的数据包保存在接收缓存里，然后应用程序主动从缓存读取收到的分组。这样一来，如果TCP接收数据包到缓存的速度大于应用程序从缓存中读取数据包的速度，多个包就会被缓存，应用程序就有可能读取到多个首尾相接粘到一起的包。
# 3.Q：什么时候需要处理粘包现象？
如果发送方发送的多组数据本来就是同一块数据的不同部分，比如说一个文件被分成多个部分发送，这时当然不需要处理粘包现象如果多个分组毫不相干，甚至是并列关系，那么这个时候就一定要处理粘包现象了
# 4.Q：如何处理粘包现象？
（1）发送方对于发送方造成的粘包问题，可以通过关闭Nagle算法来解决，使用TCP_NODELAY选项来关闭算法。
（2）接收方接收方没有办法来处理粘包现象，只能将问题交给应用层来处理。
（3）应用层应用层的解决办法简单可行，不仅能解决接收方的粘包问题，还可以解决发送方的粘包问题。
解决办法：**循环处理，**应用程序从接收缓存中读取分组时，读完一条数据，就应该循环读取下一条数据，直到所有数据都被处理完成，但是如何判断每条数据的长度呢？
**格式化数据：每条数据有固定的格式（开始符，结束符），这种方法简单易行，但是选择开始符和结束符时一定要确保每条数据的内部不包含开始符和结束符。**
**发送长度：发送每条数据时，将数据的长度一并发送，例如规定数据的前4位是数据的长度，应用层在处理时可以根据长度来判断每个分组的开始和结束位置。**
5.Q：UDP会不会产生粘包问题呢？
TCP为了保证可靠传输并减少额外的开销（每次发包都要验证），采用了基于流的传输，基于流的传输不认为消息是一条一条的，是无保护消息边界的（保护消息边界：指传输协议把数据当做一条独立的消息在网上传输，接收端一次只能接受一条独立的消息）。UDP则是面向消息传输的，是有保护消息边界的，接收方一次只接受一条独立的信息，所以不存在粘包问题。

# HTTP

* 概述

  HTTP协议是应用层协议，定义Web客户端如何从Web服务器请求Web页面，以及服务器如何把Web页面传送给客户端。HTTP协议采用了请求/响应模型。客户端向服务器发送一个请求报文，请求报文包含请求的方法、URL、协议版本、请求头部和请求数据。服务器以一个状态行作为响应，响应的内容包括协议的版本、成功或者错误代码、服务器信息、响应头部和响应数据。

* HTTP请求方法

  HTTP/1.1协议中共定义了八种方法，来表明Request-URL指定的资源不同操作方式

  HTTP1.0d定义了三种请求方法：GET,POST和HEAD方法

  HTTP1.1新增了五种请求方法：OPTIONS,PUT,DELETE,TRACE和CONNECT

  1. **GET**

     用于从指定资源请求数据

  2. **POST**

     向指定资源提交数据进行处理请求（例如提交表单或者上传文件）

  3. **HAED**

     HEAD /users 将发出相同的请求，区别是获得的响应中没有响应主体。HEAD 请求对于在实际发出 GET 请求之前（例如在下载大文件或响应正文之前）检查 GET 请求将返回的内容很有用。

  4. **PUT**

     向指定资源位置上传其最新内容

  5. **DELETE**

     请求服务器删除指定的资源

  6. **TRACE** 

     回显服务器收到的请求，主要用于测试或诊断

  7. **OPTIONS**

     这个方法可使服务器传回该资源所支持的所有HTTP请求方法

  8. **CONNECT**

     http/1.1协议中预留给能够将连接改为管道方式的代理服务器，通常用于SSL加密服务器的链接（经由非加密的HTTP代理服务器）

* 请求返回状态码
  * **200** OK 当您的操作将在响应正文中返回数据时，出现此结果。
  * **204** No Content 当您的操作成功，但不在响应正文中返回数据时，出现此结果
  * **304** Not Modified（重定向） 当测试实体自上次检索以来是否被修改时，出现此结果。
  * **403** Forbidden  客户端错误
  * **401** Unauthorized 客户端错误
  * **413** Payload Too Large（客户端错误） 当请求长度过长时，出现此结果
  * **400** BadRequest（客户端错误） 当参数无效时，出现此结果
  * **404** Not Found（客户端错误） 当资源不存在时，出现此结果
  * **405** Method Not Allowed（客户端错误）由于方法和资源组合不正确而出现此错误。 例如，您不能对一个实体集合使用 DELETE 或 PATCH
  * **412** Precondition Failed 客户端错误
  * **501** Not Implemented（服务器错误） 当未实施某个请求的操作时，出现此结果
  * **503** Service Unavailable（服务器错误） 当 Web API 服务不可用时，出现此结果

* GET与POST

  * 填充位置：“get”方法提交的数据会直接填充在请求报文的URL上,“post”方法提交的数据会附在正文上，一般请求正文的长度是没有限制的
  * 刷新：GET无害，POST数据会被重新提交
  * 书签：可收藏为书签，不可收藏为书签
  * 缓存：能被缓存，不能被缓存
  * 历史：参数保留在浏览器历史中，参数不会保存在浏览器历史中
  * 数据长度：有限制，无限制
  * 安全性：较差，较好
  * 可见性：数据在URL中对所有人可见，数据不会显示在url中

  

# HTTPS
可以说是安全版的HTTP协议，即在传统的HTTP和TCP之间加了一层用于加密解密的SSL/TLS层。HTTP默认使用80端口，HTTPS默认使用443端口。能够实现信息加密传输，校验机制，身份证书，降低了通信内容被窃听被篡改或者冒充他人通信的风险。
# HTTPS和HTTP的区别主要如下：
1、https协议需要到ca申请证书，一般免费证书较少，因而需要一定费用。
2、http是超文本传输协议，信息是明文传输，https则是具有安全性的ssl加密传输协议。
3、http和https使用的是完全不同的连接方式，用的端口也不一样，前者是80，后者是443。
4、http的连接很简单，是无状态的；HTTPS协议是由SSL+HTTP协议构建的可进行加密传输、身份认证的网络协议，比http协议安全。

# HTTPS在客户端和web服务器之间通信时的步骤
（1）客户使用https的URL访问Web服务器，要求与Web服务器建立SSL连接。
（2）Web服务器收到客户端请求后，会将网站的证书信息（证书中包含公钥）传送一份给客户端。
（3）客户端的浏览器与Web服务器开始协商SSL连接的安全等级，也就是信息加密的等级。
（4）客户端的浏览器根据双方同意的安全等级，建立会话密钥，然后利用网站的公钥将会话密钥加密，并传送给网站。
（5）Web服务器利用自己的私钥解密出会话密钥。
（6）web服务器利用会话密钥加密与客户端之间的通信。

# HTTPS安全性保证
* 对称加密
    * 加密和解密使用的密钥是同一个
    * 优点：计算量小，算法速度快，加密效率高 缺点：密钥容易泄漏。不同的会话需要不同的密钥，管理起来很费劲   
    非对称加密：

    * 需要公钥和私钥，公钥用来加密，私钥用来解密
    *  常用算法：RSA，ECC，DSA   

* **数字签名**：
  作用：保证数据的完整性，不被篡改或破坏
  生成过程：先对将要传输的信息进行hash，得到一串独一无二的字符，通常把hash之后的内容称为信息摘要message digest，
  hash是不可逆的无法根据信息摘要推测出原文，不同的原文，会造成不同的hash结果，并且结果的差异是巨大甚至毫无规律的，。
  用自己的私钥进行加密，得到数字签名。
  接受方先用公钥进行解密，之后将信息原文进行哈希得到自己hash的信息摘要，再与数字签名解码得到的hash摘要进行比较，如果相同说明原文信息是完整的。

* **数字证书**：
  作用：保证公钥没有没伪造，保证是真实的服务器发送的作者：小谷围coder

  权威CA使用私钥将网站A的信息和消息摘要（签名S）进行加密打包形成数字证书。公钥给[客户端]()。

  网站A将自己的信息和数字证书发给[客户端]()，[客户端]()用CA的公钥对数字证书进行解密，得到签名S，与手动将网站的信息进行消息摘要得到的结果S*进行对比，如果签名一致就证明网站A可以信任。

# http1.0与http1.1
* 长连接
  HTTP1.1支持长连接，在一个TCP连接上可以传送多个HTTP请求和响应，减少了建立和关闭连接的消耗和延迟，在HTTP1.1中默认开启长连接keep-alive，一定程度上弥补了HTTP1.0每次请求都要创建连接的缺点。

* 缓存处理
  HTTP/1.1引入了更多的缓存控制策略

* HOST头域
  HTTP1.0中认为每台服务器都绑定一个唯一的IP地址，因此，请求消息中的URL并没有传递主机名（hostname）。但随着虚拟主机技术的发展，在一台物理服务器上可以存在多个虚拟主机（Multi-homed Web Servers），并且它们共享一个IP地址。HTTP1.1的请求消息和响应消息都应支持Host头域，且请求消息中如果没有Host头域会报告一个错误（400 Bad Request）。有了Host字段，就可以将请求发往同一台服务器上的不同网站，为虚拟主机的兴起打下了基础。

* 范围请求：HTTP/1.1在请求头引入了`range`头域，它允许只请求资源的某个部分，即返回码是206（Partial Content），这样就方便了开发者自由的选择以便于充分利用带宽和连接，支持断点续传。

* 管道机制：HTTP/1.1中引入了管道机制（pipelining）,即在同一个TCP连接中，客户端可以**同时**发送多个请求。

* 错误状态管理，HTTP/1.1新增了24个错误状态响应码，如409（Conflict）表示请求的资源与资源的当前状态发生冲突；410（Gone）表示服务器上的某个资源被永久性的删除

  

# http1.1与http2.0
* 多路复用
HTTP2.0使用了多路复用的技术，做到同一个连接并发处理多个请求-响应消息
* 二进制分帧：HTTP/1.1的头信息是文本（ASCII编码），数据体可以是文本，也可以是二进制；HTTP/2 头信息和数据体都是二进制，统称为“帧”：头信息帧和数据帧；
* 服务器推送
HTTP/2 允许服务器未经请求，主动向客户端发送资源，这叫做服务器推送（server push）。常见场景是客户端请求一个网页，这个网页里面包含很多静态资源。正常情况下，客户端必须收到网页后，解析HTML源码，发现有静态资源，再发出静态资源请求。其实，服务器可以预期到客户端请求网页后，很可能会再请求静态资源，所以就主动把这些静态资源随着网页一起发给客户端了。
*  首部压缩：HTTP 协议不带有状态，每次请求都必须附上所有信息。所以，请求的很多字段都是重复的，，一模一样的内容，每次请求都必须附带，这会浪费很多带宽，也影响速度。HTTP/2 对这一点做了优化，引入了头信息压缩机制（header compression）。一方面，头信息压缩后再发送（SPDY 使用的是通用的DEFLATE 算法，而 HTTP/2 则使用了专门为首部压缩而设计的 HPACK 算法）。；另一方面，客户端和服务器同时维护一张头信息表，所有字段都会存入这个表，生成一个索引号，以后就不发送同样字段了，只发送索引号，这样就提高速度了。

* 数据流：因为 HTTP/2 的数据包是不按顺序发送的，同一个连接里面连续的数据包，可能属于不同的回应。因此，必须要对数据包做标记，指出它属于哪个回应。HTTP/2 将每个请求或回应的所有数据包，称为一个数据流（stream）。每个数据流都有一个独一无二的**编号**。数据包发送的时候，都必须标记数据流ID，用来区分它属于哪个数据流。另外还规定，客户端发出的数据流，ID一律为奇数，服务器发出的，ID为偶数。数据流发送到一半的时候，客户端和服务器都可以发送信号（`RST_STREAM`帧），取消这个数据流。HTTP/1.1取消数据流的唯一方法，就是关闭TCP连接。这就是说，HTTP/2 可以取消某一次请求，同时保证TCP连接还打开着，可以被其他请求使用。客户端还可以指定数据流的优先级。优先级越高，服务器就会越早回应。

# 在浏览器输入URL回车后发生了什么

1. **URL解析**

   首先判断你输入的是一个合法的 URL 还是一个待搜索的关键词，并且根据你输入的内容进行自动完成字符编码等操作，由于安全隐患，会使用 HSTS 强制客户端使用 HTTPS 访问页面。浏览器还会进行一些额外的操作，比如安全检查、访问限制。

2. **域名解析**

   根据域名去查询对应的IP

   * 浏览器缓存：浏览器会按照一定的频率缓存 DNS 记录。
   * 操作系统缓存：如果浏览器缓存中找不到需要的 DNS 记录，那就去操作系统中找。
   * 路由缓存：路由器也有 DNS 缓存。
   * 本地域名服务器：向本地域名服务器进行迭代查询
   * 根域名服务器：ISP 的 DNS 服务器还找不到的话，它就会向根服务器发出请求，进行递归查询（DNS 服务器先问根域名服务器.com 域名服务器的 IP 地址，然后再问.baidu 域名服务器，依次类推）

3.  **tcp进行三次握手建立链接**

   ```
   1、第一次握手：客户端给服务端发一个 连接请求的数据包，此时SYN=1,ACK=0,初始化序列号 x。此时客户端处于 SYN_Send 状态。
     2、第二次握手：服务器收到客户端的连接请求报文之后，如果同意建立连接，会向客户端发送连接确认报数据包，SYN=1,ACK=1,确认号为x+1,并且也是指定了自己的初始化序列号 y,此时服务器处于 SYN_REVD 的状态。
     3、第三次握手：客户端收到连接确认报文之后，还会向服务器发送确认，确认号为y+1，序列号为x+1,此时客户端处于 establised 状态。
     4、服务器收到 ACK 报文之后，也处于 establised 状态，此时，双方以建立起了链接。
     * 建立三次握手的原因：
     1. 确认双方的接收能力和发送能力是否正常
     2. 初始化序列号为可靠传输做准备
     3. 如果是https连接，还会进行数字证书的验证和会话秘钥的生成
     4. 防止失效的连接请求到达服务器，让服务器错误打开连接
   ```

4. **发送http请求**

   * 请求报文由请求行（request line）、请求头（header）、请求体三个部分组成
     * 请求行包含请求方法，URL，协议版本
     * 请求头部包含请求的附加信息，由关键字/值对组成，请求头部通知服务器有关于客户端请求的信息。它包含许多有关的客户端环境和请求正文的有用信息。
     * 请求体，**可以承载多个请求参数的数据，包含回车符、换行符和请求数据，并不是所有请求都具有请求数据。**

5. **服务器处理请求并返回HTTP响应报文**

   * 响应报文由响应行（request line）、响应头部（header）、响应主体三个部分组成
     * 响应行包含：协议版本，状态码，状态码描述
     * 响应头部包含响应报文的附件信息，由名/值对组成
     * 响应主体包含回车符、换行符和响应返回数据，并不是所有响应报文都有响应数据

   1. 服务器

      服务器是网络环境中的高性能计算机，它侦听网络上的其他计算机（客户机）提交的服务请求，并提供相应的服务，比如网页服务、文件下载服务、邮件服务、视频服务。

      

6. **浏览器解析渲染页面**

7. **断开链接**

 刚开始双方都处于 establised 状态，假如是客户端先发起关闭请求，则：
    1、第一次挥手：客户端发送一个 FIN 报文，报文中会指定一个序列号。此时客户端处于FIN_WAIT1状态。
    2、第二次握手：服务端收到 FIN 之后，会发送 ACK 报文，且把客户端的序列号值 + 1 作为 ACK 报文的序列号值，表明已经收到客户端的报文了，此时服务端处于 CLOSE_WAIT状态。
    3、第三次挥手：如果服务端也想断开连接了，和客户端的第一次挥手一样，发给 FIN 报文，且指定一个序列号。此时服务端处于 LAST_ACK 的状态。
    4、第四次挥手：客户端收到 FIN 之后，一样发送一个 ACK 报文作为应答，且把服务端的序列号值 + 1 作为自己 ACK 报文的序列号值，此时客户端处于 TIME_WAIT 状态。需要过一阵子以确保服务端收到自己的 ACK 报文之后才会进入 CLOSED 状态
    5、服务端收到 ACK 报文之后，就处于关闭连接了，处于 CLOSED 状态。

# 路由器分组转发的流程

1. 从数据报的首部提取目的主机的 IP 地址 D，得到目的网络地址 N
2. 若 N 就是与此路由器直接相连的某个网络地址，则进行直接交付
3. 若路由表中有目的地址为 D 的特定主机路由，则把数据报传送给表中所指明的下一跳路由器
4. 若路由表中有到达网络 N 的路由，则把数据报传送给路由表中所指明的下一跳路由器
5. 若路由表中有一个默认路由，则把数据报传送给路由表中所指明的默认路由器
6. 报告转发分组出错

# 路由选择协议

1. 内部网关协议IGP:RIP(内部网关协议)，OSPF(开放最短路径优先)
2. 外部网关协议EGP:(BGP)

### 1. 内部网关协议 RIP

RIP 是一种分布式的基于距离向量的路由选择协议。距离是指跳数，直接相连的路由器跳数为 1，跳数最多为 15，超过 15 表示不可达。

RIP 按固定的时间间隔仅和相邻路由器交换自己的路由表，经过若干次交换之后，所有路由器最终会知道到达本自治系统中任何一个网络的最短距离和下一跳路由器地址。

距离向量算法：

1. 对地址为 X 的相邻路由器发来的 RIP 报文，先修改报文中的所有项目，把下一跳字段中的地址改为 X，并把所有的距离字段加 1；
2. 对修改后的 RIP 报文中的每一个项目，进行以下步骤：

 - 若原来的路由表中没有目的网络 N，则把该项目添加到路由表中；
 - 否则：若下一跳路由器地址是 X，则把收到的项目替换原来路由表中的项目；否则：若收到的项目中的距离 d 小于路由表中的距离，则进行更新（例如原始路由表项为 Net2, 5, P，新表项为 Net2, 4, X，则更新）；否则什么也不做。

3. 若 3 分钟还没有收到相邻路由器的更新路由表，则把该相邻路由器标为不可达，即把距离置为 16。

RIP 协议实现简单，开销小，但是 RIP 能使用的最大距离为 15，限制了网络的规模。并且当网络出现故障时，要经过比较长的时间才能将此消息传送到所有路由器。

### 2. 内部网关协议 OSPF

开放最短路径优先 OSPF，是为了克服 RIP 的缺点而开发出来的。

开放表示 OSPF 不受某一家厂商控制，而是公开发表的；最短路径优先表示使用了 Dijkstra 提出的最短路径算法 SPF。

OSPF 具有以下特点：

- 向本自治系统中的所有路由器发送信息，这种方法是洪泛法。
- 发送的信息就是与相邻路由器的链路状态，链路状态包括与哪些路由器相连以及链路的度量，度量用费用、距离、时延、带宽等来表示。
- 只有当链路状态发生变化时，路由器才会发送信息。

所有路由器都具有全网的拓扑结构图，并且是一致的。相比于 RIP，OSPF 的更新过程收敛的很快。

## 分组网间探测 PING

PING 是 ICMP 的一个重要应用，主要用来测试两台主机之间的连通性。

Ping 发送的 IP 数据报封装的是无法交付的 UDP 用户数据报。

Ping 的过程：

1. 源主机向目的主机发送一连串的 IP 数据报。第一个数据报 P1 的生存时间 TTL 设置为 1，但 P1 到达路径上的第一个路由器 R1 时，R1 收下它并把 TTL 减 1，此时 TTL 等于 0，R1 就把 P1 丢弃，并向源主机发送一个 ICMP 时间超过差错报告报文；
2. 源主机接着发送第二个数据报 P2，并把 TTL 设置为 2。P2 先到达 R1，R1 收下后把 TTL 减 1 再转发给 R2，R2 收下后也把 TTL 减 1，由于此时 TTL 等于 0，R2 就丢弃 P2，并向源主机发送一个 ICMP 时间超过差错报文。
3. 不断执行这样的步骤，直到最后一个数据报刚刚到达目的主机，主机不转发数据报，也不把 TTL 值减 1。但是因为数据报封装的是无法交付的 UDP，因此目的主机要向源主机发送 ICMP 终点不可达差错报告报文。
4. 之后源主机知道了到达目的主机所经过的路由器 IP 地址以及到达每个路由器的往返时间。